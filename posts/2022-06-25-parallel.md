---
toc: true
comments: true
layout: post
description: blog.
categories: [markdown]
title: Parallel in Deep learning
---
# Parallel in Deep learning
在cluster上用多个GPU进行训练，减少训练的时间；另外随着batch Size 增加，需要的内存也越来越大，在一块GPU上不能进行训练，因此如何用多个GPU进行训练也是一个需求。

## DataParallel in Pytorch
虽然DataParallel不被推荐使用，即使是在一个node中[^1]，但是因为[`DistributedDataParallel`](https://pytorch.org/docs/stable/generated/torch.nn.parallel.DistributedDataParallel.html#torch.nn.parallel.DistributedDataParallel)有一些问题，如windows不支持，所以DataParallel还是可以用的[^2]。

不推荐的主要原因是由于模型要在多个node中复制。

另外的还有**mp.spawn**来做并行，感觉需要的代码更多[^3]。

## DistributedDataParallel in Pytorch

DistributedDataParallel 是只用Pytorch的基础上利用较多的，目前很多这样的博客，缺点就是配置起来比较困难[^7]，具体的可以参考blog[^8]。

## Pytorch Lightning

是独立于Pytorch的一个库[^6]，因此对于分布配置时并不用考虑，可以直接从pytorch的模型到pytorch lightning[^5]。

##  Horovod
Horovod[^9] 可以认为是一个分布式训练的框架，可以支持多种工具，如TensorFlow, Keras, PyTorch, and Apache MXNet。
目前在Pytorch Lightning中，也可以支持Horovod[^11]，对于pytorch，也可以自己配置[^10]。

## Batch size

Batch size 是一个重要的问题，因为batch size的大小影响训练的精度，实际上的batch size是多少跟采用哪种并行方式有关，可以参考视频中的计算方式[^2]。

考虑到Batch size 的变化，learning rate也需要跟着变化，要不然用DDP的accuracy会降低[^4]。


## Footnotes
[^1]: https://pytorch.org/docs/stable/generated/torch.nn.DataParallel.html.
[^2]: https://www.youtube.com/watch?v=a6_pY9WwqdQ.
[^3]: https://zhuanlan.zhihu.com/p/336863012.
[^4]:https://discuss.pytorch.org/t/should-we-split-batch-size-according-to-ngpu-per-node-when-distributeddataparallel/72769/6.
[^5]:https://www.youtube.com/watch?v=DbESHcCoWbM&t=1678s.
[^6]:https://zhuanlan.zhihu.com/p/319810661.
[^7]:https://zhuanlan.zhihu.com/p/206467852.
[^8]:https://shomy.top/2022/01/05/torch-ddp-intro/.
[^9]: https://github.com/horovod/horovod.
[^10]:https://zhuanlan.zhihu.com/p/264778072.
[^11]:https://horovod.readthedocs.io/en/stable/pytorch.html.

