---
toc: true
layout: post
description: blog.
categories: [markdown]
title: Parallel in Deep learning
---
# Parallel in Deep learning
在cluster上用多个GPU进行训练，减少训练的时间；另外随着batch Size 增加，需要的内存也越来越大，在一块GPU上不能进行训练，因此如何用多个GPU进行训练也是一个需求。

## DataParallel in Pytorch
虽然DataParallel不被推荐使用，即使是在一个node中[^1]，但是因为[`DistributedDataParallel`](https://pytorch.org/docs/stable/generated/torch.nn.parallel.DistributedDataParallel.html#torch.nn.parallel.DistributedDataParallel)有一些问题，如windows不支持，所以DataParallel还是可以用的[^2]。

## DistributedDataParallel in Pytorch



## Pytorch Lightning



## Batch size











## Footnotes
[^1]: https://pytorch.org/docs/stable/generated/torch.nn.DataParallel.html.
[^2]: https://www.youtube.com/watch?v=a6_pY9WwqdQ.

