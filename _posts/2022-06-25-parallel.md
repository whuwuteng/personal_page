---
toc: true
layout: post
description: blog.
categories: [markdown]
title: Parallel in Deep learning
---
# Parallel in Deep learning
在cluster上用多个GPU进行训练，减少训练的时间；另外随着batch Size 增加，需要的内存也越来越大，在一块GPU上不能进行训练，因此如何用多个GPU进行训练也是一个需求。

## DataParallel in Pytorch
虽然DataParallel不被推荐使用，即使是在一个node中[^1]，但是因为[`DistributedDataParallel`](https://pytorch.org/docs/stable/generated/torch.nn.parallel.DistributedDataParallel.html#torch.nn.parallel.DistributedDataParallel)有一些问题，如windows不支持，所以DataParallel还是可以用的[^2]。

不推荐的主要原因是由于模型要在多个node中复制。

另外的还有**mp.spawn**来做并行，感觉需要的代码更多[^3]。

## DistributedDataParallel in Pytorch

DistributedDataParallel 是只用Pytorch的基础上利用较多的，目前很多这样的博客，缺点就是配置起来比较困难[^7]。

## Pytorch Lightning

是独立于Pytorch的一个库[^6]，因此对于分布配置时并不用考虑，可以直接从pytorch的模型到pytorch lightning[^5]。

## Batch size

Batch size 是一个重要的问题，因为batch size的大小影响训练的精度，实际上的batch size是多少跟采用哪种并行方式有关，可以参考视频中的计算方式[^2]。

考虑到Batch size 的变化，learning rate也需要跟着变化，要不然用DDP的accuracy会降低[^4]。



## Footnotes
[^1]: https://pytorch.org/docs/stable/generated/torch.nn.DataParallel.html.
[^2]: https://www.youtube.com/watch?v=a6_pY9WwqdQ.
[^3]: https://zhuanlan.zhihu.com/p/336863012.
[^4]:https://discuss.pytorch.org/t/should-we-split-batch-size-according-to-ngpu-per-node-when-distributeddataparallel/72769/6.
[^5]:https://www.youtube.com/watch?v=DbESHcCoWbM&t=1678s.
[^6]:https://zhuanlan.zhihu.com/p/319810661.
[^7]:https://zhuanlan.zhihu.com/p/206467852.

