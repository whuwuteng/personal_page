<article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Parallel in Deep learning</h1><p class="page-description">blog.</p><p class="post-meta post-meta-title"><time class="dt-published" datetime="2022-06-25T00:00:00-05:00" itemprop="datePublished">
        Jun 25, 2022
      </time>
       • <span class="read-time" title="Estimated read time">
    
    
      1 min read
    
</span></p>

    
      <p class="category-tags"><i class="fas fa-tags category-tags-icon"></i></i> 
      
        <a class="category-tags-link" href="/personal_page/categories/#markdown">markdown</a>
        
      
      </p>
    

    </header>

  <div class="post-content e-content" itemprop="articleBody">
    <ul class="section-nav">
<li class="toc-entry toc-h1"><a href="#parallel-in-deep-learning">Parallel in Deep learning</a>
<ul>
<li class="toc-entry toc-h2"><a href="#dataparallel-in-pytorch">DataParallel in Pytorch</a></li>
<li class="toc-entry toc-h2"><a href="#distributeddataparallel-in-pytorch">DistributedDataParallel in Pytorch</a></li>
<li class="toc-entry toc-h2"><a href="#pytorch-lightning">Pytorch Lightning</a></li>
<li class="toc-entry toc-h2"><a href="#horovod">Horovod</a></li>
<li class="toc-entry toc-h2"><a href="#batch-size">Batch size</a></li>
<li class="toc-entry toc-h2"><a href="#footnotes">Footnotes</a></li>
</ul>
</li>
</ul><h1 id="parallel-in-deep-learning">
<a class="anchor" href="#parallel-in-deep-learning" aria-hidden="true"><span class="octicon octicon-link"></span></a>Parallel in Deep learning</h1>
<p>在cluster上用多个GPU进行训练，减少训练的时间；另外随着batch Size 增加，需要的内存也越来越大，在一块GPU上不能进行训练，因此如何用多个GPU进行训练也是一个需求。</p>

<h2 id="dataparallel-in-pytorch">
<a class="anchor" href="#dataparallel-in-pytorch" aria-hidden="true"><span class="octicon octicon-link"></span></a>DataParallel in Pytorch</h2>
<p>虽然DataParallel不被推荐使用，即使是在一个node中<sup id="fnref:1" role="doc-noteref"><a href="#fn:1" class="footnote" rel="footnote">1</a></sup>，但是因为<a href="https://pytorch.org/docs/stable/generated/torch.nn.parallel.DistributedDataParallel.html#torch.nn.parallel.DistributedDataParallel"><code class="language-plaintext highlighter-rouge">DistributedDataParallel</code></a>有一些问题，如windows不支持，所以DataParallel还是可以用的<sup id="fnref:2" role="doc-noteref"><a href="#fn:2" class="footnote" rel="footnote">2</a></sup>。</p>

<p>不推荐的主要原因是由于模型要在多个node中复制。</p>

<p>另外的还有<strong>mp.spawn</strong>来做并行，感觉需要的代码更多<sup id="fnref:3" role="doc-noteref"><a href="#fn:3" class="footnote" rel="footnote">3</a></sup>。</p>

<h2 id="distributeddataparallel-in-pytorch">
<a class="anchor" href="#distributeddataparallel-in-pytorch" aria-hidden="true"><span class="octicon octicon-link"></span></a>DistributedDataParallel in Pytorch</h2>

<p>DistributedDataParallel 是只用Pytorch的基础上利用较多的，目前很多这样的博客，缺点就是配置起来比较困难<sup id="fnref:7" role="doc-noteref"><a href="#fn:7" class="footnote" rel="footnote">4</a></sup>，具体的可以参考blog<sup id="fnref:8" role="doc-noteref"><a href="#fn:8" class="footnote" rel="footnote">5</a></sup>。</p>

<h2 id="pytorch-lightning">
<a class="anchor" href="#pytorch-lightning" aria-hidden="true"><span class="octicon octicon-link"></span></a>Pytorch Lightning</h2>

<p>是独立于Pytorch的一个库<sup id="fnref:6" role="doc-noteref"><a href="#fn:6" class="footnote" rel="footnote">6</a></sup>，因此对于分布配置时并不用考虑，可以直接从pytorch的模型到pytorch lightning<sup id="fnref:5" role="doc-noteref"><a href="#fn:5" class="footnote" rel="footnote">7</a></sup>。</p>

<h2 id="horovod">
<a class="anchor" href="#horovod" aria-hidden="true"><span class="octicon octicon-link"></span></a>Horovod</h2>
<p>Horovod<sup id="fnref:9" role="doc-noteref"><a href="#fn:9" class="footnote" rel="footnote">8</a></sup> 可以认为是一个分布式训练的框架，可以支持多种工具，如TensorFlow, Keras, PyTorch, and Apache MXNet。
目前在Pytorch Lightning中，也可以支持Horovod<sup id="fnref:11" role="doc-noteref"><a href="#fn:11" class="footnote" rel="footnote">9</a></sup>，对于pytorch，也可以自己配置<sup id="fnref:10" role="doc-noteref"><a href="#fn:10" class="footnote" rel="footnote">10</a></sup>。</p>

<h2 id="batch-size">
<a class="anchor" href="#batch-size" aria-hidden="true"><span class="octicon octicon-link"></span></a>Batch size</h2>

<p>Batch size 是一个重要的问题，因为batch size的大小影响训练的精度，实际上的batch size是多少跟采用哪种并行方式有关，可以参考视频中的计算方式<sup id="fnref:2:1" role="doc-noteref"><a href="#fn:2" class="footnote" rel="footnote">2</a></sup>。</p>

<p>考虑到Batch size 的变化，learning rate也需要跟着变化，要不然用DDP的accuracy会降低<sup id="fnref:4" role="doc-noteref"><a href="#fn:4" class="footnote" rel="footnote">11</a></sup>。</p>

<h2 id="footnotes">
<a class="anchor" href="#footnotes" aria-hidden="true"><span class="octicon octicon-link"></span></a>Footnotes</h2>

<div class="footnotes" role="doc-endnotes">
  <ol>
    <li id="fn:1" role="doc-endnote">
      <p>https://pytorch.org/docs/stable/generated/torch.nn.DataParallel.html. <a href="#fnref:1" class="reversefootnote" role="doc-backlink">↩</a></p>
    </li>
    <li id="fn:2" role="doc-endnote">
      <p>https://www.youtube.com/watch?v=a6_pY9WwqdQ. <a href="#fnref:2" class="reversefootnote" role="doc-backlink">↩</a> <a href="#fnref:2:1" class="reversefootnote" role="doc-backlink">↩<sup>2</sup></a></p>
    </li>
    <li id="fn:3" role="doc-endnote">
      <p>https://zhuanlan.zhihu.com/p/336863012. <a href="#fnref:3" class="reversefootnote" role="doc-backlink">↩</a></p>
    </li>
    <li id="fn:7" role="doc-endnote">
      <p>https://zhuanlan.zhihu.com/p/206467852. <a href="#fnref:7" class="reversefootnote" role="doc-backlink">↩</a></p>
    </li>
    <li id="fn:8" role="doc-endnote">
      <p>https://shomy.top/2022/01/05/torch-ddp-intro/. <a href="#fnref:8" class="reversefootnote" role="doc-backlink">↩</a></p>
    </li>
    <li id="fn:6" role="doc-endnote">
      <p>https://zhuanlan.zhihu.com/p/319810661. <a href="#fnref:6" class="reversefootnote" role="doc-backlink">↩</a></p>
    </li>
    <li id="fn:5" role="doc-endnote">
      <p>https://www.youtube.com/watch?v=DbESHcCoWbM&amp;t=1678s. <a href="#fnref:5" class="reversefootnote" role="doc-backlink">↩</a></p>
    </li>
    <li id="fn:9" role="doc-endnote">
      <p>https://github.com/horovod/horovod. <a href="#fnref:9" class="reversefootnote" role="doc-backlink">↩</a></p>
    </li>
    <li id="fn:11" role="doc-endnote">
      <p>https://horovod.readthedocs.io/en/stable/pytorch.html. <a href="#fnref:11" class="reversefootnote" role="doc-backlink">↩</a></p>
    </li>
    <li id="fn:10" role="doc-endnote">
      <p>https://zhuanlan.zhihu.com/p/264778072. <a href="#fnref:10" class="reversefootnote" role="doc-backlink">↩</a></p>
    </li>
    <li id="fn:4" role="doc-endnote">
      <p>https://discuss.pytorch.org/t/should-we-split-batch-size-according-to-ngpu-per-node-when-distributeddataparallel/72769/6. <a href="#fnref:4" class="reversefootnote" role="doc-backlink">↩</a></p>
    </li>
  </ol>
</div>

  </div><!-- from https://github.com/utterance/utterances -->
<script src="https://utteranc.es/client.js"
        repo="whuwuteng/personal_page"
        issue-term="title"
        label="blogpost-comment"
        theme="github-light"
        crossorigin="anonymous"
        async>
</script><a class="u-url" href="/personal_page/markdown/2022/06/25/parallel.html" hidden></a>
</article>